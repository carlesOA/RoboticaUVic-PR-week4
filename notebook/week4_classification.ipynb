{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Welcome! In this notebook we will see how to use sklearn to perform classification\n",
      "#first let's import the necessary libraries:\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "#sklearn is divided in several sub-libraries for the different functionality, for\n",
      "#a complete overview see the API here: http://scikit-learn.org/stable/modules/classes.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Next let's load some data to play with. In this notebook we will use\n",
      "#the Abalone dataset from the UCI repository. The dataset should be put \n",
      "#in the same folder as the notebook. \n",
      "\n",
      "# with this dictionary we will encode the categorical feature 'gender'.\n",
      "cat_gender = {'I':np.array([1,0,0]), 'M':np.array([0,1,0]), 'F':np.array([0,0,1])}\n",
      "#loading the data and massaging it to become usable \n",
      "abalone_data = [x.strip().split(',') for x in open('abalone/abalone.data').readlines()] \n",
      "#Construct an array with the categorical feature (first one) and the remaining features\n",
      "abalone_data = np.array([np.hstack((cat_gender[x[0]],np.array(map(float,x[1:])))) for x in abalone_data])\n",
      "#Take last column as label\n",
      "abalone_labels = abalone_data[:,-1].astype('int')\n",
      "#The remaining columns are the data\n",
      "abalone_data = abalone_data[:,:-1]\n",
      "#We split the data randomly between train and test\n",
      "naba= abalone_labels.shape[0]\n",
      "sel = range(naba)\n",
      "np.random.seed(25)\n",
      "np.random.shuffle(sel)\n",
      "aba_train_data = abalone_data[sel[:naba/2],:]\n",
      "aba_train_labs = abalone_labels[sel[:naba/2]]\n",
      "aba_test_data = abalone_data[sel[naba/2:],:]\n",
      "aba_test_labs = abalone_labels[sel[naba/2:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#K-NN\n",
      "#We define a function to run the K-NN classifier with a range of 'k' values\n",
      "def runknn(tr_data, tr_labels, te_data, te_labels):\n",
      "    for k in range(1,15,2):\n",
      "        #Here we use the KNN function from scikit learn (in the\n",
      "        #homework you will have to implement it yourselves) \n",
      "        #First we create a classifier object\n",
      "        knn = KNeighborsClassifier(k)\n",
      "        #Then we \"train\" it with the training data\n",
      "        knn.fit(aba_train_data, aba_train_labs)\n",
      "        #Finally we compute the accuracy of the test data\n",
      "        acc = knn.score(aba_test_data, aba_test_labs)\n",
      "        print 'Accuracy (k=%d): %.4f'%(k,acc)\n",
      "#We call the function with the train and test splits\n",
      "runknn(aba_train_data, aba_train_labs, aba_test_data, aba_test_labs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Logistic Regression\n",
      "#Similarly as in KNN, we first create a classifier object, with C=1\n",
      "LogReg = LogisticRegression(C=1)\n",
      "#Then we train it with the training data\n",
      "LogReg.fit(aba_train_data, aba_train_labs)\n",
      "#And finally use the score function to get the accuracy\n",
      "print 'Log Reg accuracy', LogReg.score(aba_test_data, aba_test_labs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#But, is C=1 the best regularization parameter? We will\n",
      "#find out with cross-validation + grid search\n",
      "#sklearn has a handy function for that too:\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "#we define the search space for the C parameter\n",
      "parameters = {'C':[10**i for i in range(-7,7)]}\n",
      "#and create a grid search object with the type of classifier we want to\n",
      "#use and the parameters\n",
      "clf = GridSearchCV(LogisticRegression(), parameters, verbose=False)\n",
      "#and we train the classifier with grid search (this will use cross-validation\n",
      "#to select the best parameter)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'Log Reg with Grid search', clf.score(aba_test_data, aba_test_labs), 'with C =', clf.best_params_['C']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Linear Support Vector Machine\n",
      "#The interface for the different classifiers is very similar; the Linear SVM\n",
      "#works in the same way as the logistic regression\n",
      "clf = LinearSVC(C=1)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'Linear SVM score', clf.score(aba_test_data, aba_test_labs)\n",
      "parameters = {'C':[10**i for i in range(-3, 4)]}\n",
      "clf = GridSearchCV(LinearSVC(), parameters, verbose=False)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'Linear SVM score with Grid Search', clf.score(aba_test_data, aba_test_labs), 'with C =', clf.best_params_['C']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Now we will do the same with the kernelized SVM\n",
      "clf = SVC(C=1)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'RBF SVM score', clf.score(aba_test_data, aba_test_labs)\n",
      "#We can actually define several parameters we want to optimize over:\n",
      "parameters = {'C':[10**i for i in range(-3, 4)], 'kernel':['linear', 'rbf']}\n",
      "clf = GridSearchCV(SVC(), parameters, verbose=False)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'RBF SVM score with Grid Search', clf.score(aba_test_data, aba_test_labs), 'with C =', \\\n",
      "    parameters.best_params_['C'], 'and kernel =', parameters.best_params_['kernel']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}